# Introduction to Generative AI Studio

* <ins>What is Generative AI?</ins> - It is a type of AI that generates content for you. 

* <ins>What kind of content?</ins> - The generated content can be multi-modal, including text, images, audio, and video.

* <ins>When given a prompt or a request</ins> - Generative AI can help you achieve various tasks, such as document summarization, information extraction, code generation, marketing campaign creation, virtual assistance, and call center bot.

* ![](img/10/1.generate_new_content.PNG)
* <ins>How does AI generate new content?</ins> - It learns from a massive amount of existing content (text, audio and video), called training, which results in the creation of a <ins>“foundation model.”</ins>
    1. <ins>An large language model (LLM)</ins>, <ins>Bard</ins>, is a typical example of a foundation model.
    2. <ins>The foundation model</ins> can then be used to generate content and solve general problems, such as content extraction and document summarization. 
    3. It can also be trained further with new datasets in your field to solve specific problems, such as financial model generation and healthcare consulting. This results in the creation of a new model that is tailored to your specific needs. 

* <ins>How can you use the foundation model to power your applications, and how can you further train, or tune, the foundation model to solve a problem in your specific field?</ins>
    1. GCP provides several generative AI tools with or without an AI and machine learning background.
    2. <ins>Vertex AI</ins>

* ![](img/10/2.vertex_ai.PNG)
* <ins>Vertex AI</ins> - is an end-to-end ML development platform on GC that helps you build, deploy, and manage ML models. 
    1. An app developer or data scientist want to build an application, use <ins>Generative AI Studio</ins> to quickly prototype and customize generative AI models with no code or low code. 
    2. Data scientist or ML developer who wants to build and automate a generative AI model, you can start from <ins>Model Garden</ins>.
    3. <ins>Model Garden</ins> lets you discover and interact with Google’s foundation and third-party open source models and has built-in MLOps tools to automate the ML pipeline.

* ![](img/10/3.gen_ai_studio.PNG)
# Generative AI Studio
* It supports language, vision, and speech. 

* ![](img/10/4.language.PNG)
* <ins>Language in Generative AI Studio</ins>.
* <ins>1st feature - Design a prompt</ins>
    1. Experimenting with LLMs -> click on <ins>NEW PROMPT</ins>.
    2. In the world of Generative AI, <ins>a prompt</ins> is just a fancy name for the input text that you feed to your model. 
    * ![](img/10/5.prompt.PNG)
    3. feed your desired input text like questions and  instructions to the model.
    4. The model will then provide a response based on how you structured your prompt, therefore, the answers you get depend on the questions you ask. 
    5. The process of figuring out and designing the best input text to get the desired response back from the model is called <ins>Prompt Design</ins>, which often involves a lot of experimentation.
    * ![](img/10/5.prompt1.PNG)
    6. Example with a <ins>free-form prompt</ins> - the model outputs a useful list of items, this approach of writing a single command so that the LLM can adopt a certain behavior, is called <ins>zero shot prompting</ins>. 
    * ![](img/10/6.shot_prompt.PNG)
    7. <ins>Zero-shot prompting</ins> - is a method where the LLM is given no additional data on the specific task that it is being asked to perform. Instead, it is only given a prompt that describes the task.
    8. <ins>One-shot prompting</ins> - is a method where the LLM is given a single example of the task that it is being asked to perform. For example, if you want the LLM to write a poem, you might provide a single example poem. 
    10. <ins>Few-shot prompting</ins> - is a method where the LLM is given a small number of examples of the task that it is being asked to perform. For example, if you want the LLM to write a news article, you might give it a few news articles to read. 
    * ![](img/10/7.structure_prompt.PNG)
    * ![](img/10/7.structure_prompt1.PNG)
    11. <ins>The structured prompt</ins> contains a few different components: context (which instructs how the model should respond), examples (questions that could be answered from this passage, need to add in the corresponding answers to these questions (to demonstrate how we want the model to respond), test (sending a new question as input).
    * ![](img/10/8.best_practice.PNG)
    12. <ins>best practices around prompt design.</ins>
    13. <ins>save it</ins>, will be visible in the <ins>prompt gallery</ins>
    14. <ins>Choose different models</ins> - Each model is tuned to perform well on specific tasks. 
    *  ![](img/10/9.score.PNG)
    15. <ins>the temperature, top P, and top K</ins> - parameters all adjust the randomness of responses by controlling how the outpt tokens are selected. 
    * ![](img/10/9.score1.PNG)
    16. <ins>On the contrary</ins>, you might get some unlikely responses.
    * ![](img/10/10.randomness.PNG)
    17. By controlling the <ins>degree of randomness</ins>, get more unexpected, and some might say creative, responses. 
    * ![](img/10/11.temperature.PNG)
    18. <ins>temperature</ins> is a number used to tune the degree of randomness.
            1. <ins>Low temperature</ins>: Means to select the words that are highly possible and more predictable. Flowers and the other words that are located at the beginning of the list. better for tasks like q&a and summarization where you expect a more “predictable” answer with less variation. 
            2. <ins>High temperature</ins>: Means to select the words that have low possibility and are more unusual. Bugs and the other words that that are located at the end of the list. good to generate more “creative” or unexpected content.
    * ![](img/10/12.topk.PNG)
    19. <ins>top K</ins> - the model randomly return a word from the top K number of words in terms of possibility. <ins>For example</ins>, top 2 means you get a random word from the top 2 possible words including flowers and trees. 
            1. This approach allows the other high-scoring word a chance of being selected. 
            2. However, if the probability distribution of the words is highly skewed and you have one word that is very likely and everything else is very unlikely, this approach can result in some strange responses. 
            3. The difficulty of selecting the best top-k value, leads to <ins>another popular approach(Top P)</ins> that dynamically sets the size of the shortlist of words. 
    * ![](img/10/12.topp.PNG)
    20. <ins>Top P</ins> - the model to randomly return a word from the top P probability of words. With top P, you choose from a set of words with the sum of the likelihoods not exceeding P.
            1. <ins>For example</ins>, p of 0.75 means you sample from a set of words that have a cumulative probability greater than 0.75. In this case, it includes three words: flowers, trees, and herbs.
            2. This way, the size of the set of words can dynamically increase and decrease according to the probability distribution of the next word on the list. 
    * ![](img/10/14.model.PNG)
    21. In sum, Generative AI Studio provides a few model parameters for you to play with such as the model, temperature, top K, and top P. 
            1. Note that, you are not required to adjust them constantly, especially top k and top p. 

* ![](img/10/15.conversational.PNG)
* <ins>2nd feature - conversations context</ins> - instructs how the model should respond. 
    1. For example, Your name is Roy. You are a support technician of an IT department. You only respond with "Have you tried turning it off and on again?" to any queries. 
    2. tune the parameters, the same as designing prompt. 
    3. To to see how it works, type My computer is slow in the chat box and press enter. The AI responds: Have you tried turning it off and on again? 
    4. Exactly as you told the AI to do. 

* <ins>Google provides the APIs and SDKs to help you build your own application.</ins>

* <ins>3rd feature, tuen a language model</ins> - if there’s a way you can improve the quality of responses beyond just prompt design. 
     * ![](img/10/16.tune.PNG)
     1. prompt (instruction or some examples), send this text to the model so that it adopts the behavior that you want.
     * ![](img/10/17.prompt_design.PNG)
     2. <ins>Prompt design</ins> allows for fast experimentation and customization. Small changes in wording or word order can affect the model results in ways that aren’t totally predictable. And you can’t really fit all that many examples into a prompt. 
     3. Overcome this issues is to <ins>tune the model</ins>.
     4. <ins>what’s tuning?</ins> 
            1. one version is <ins>fine-tuning</ins> - take a model that was pretrained on a generic dataset. Make a copy of this model. Then, using those learned weights as a starting point, we re-train the model on a new domain-specific dataset. This technique has been pretty effective for lots of different use cases.
            * ![](img/10/19.fin_tunning.PNG)
            2. Try to fine tune LLMs. challenges is LLMs means name suggests, large. So updating every weight can take a long training job. Based on the cost, computation, fine-tuning a large language model might <ins>not be the best option</ins>
            * ![](img/10/20.parameter.PNG)
            3. <ins>innovative approach to tuning called parameter-efficient tuning</ins>. This is a super exciting research area that aims to reduce the challenges of fine-tuning LLMs, by only training a subset of parameters. 
            4. These parameters might be a subset of the existing model parameters. Or they could be an entirely new set of parameters. 
            5. For example, maybe you add on some additional layers to the model or an extra embedding to the prompt. 
            * ![](img/10/21.running_model.PNG)
            6. Generative AI Studio -> TUNING -> create a tuned model.
            * ![](img/10/22.example.PNG)
            7. training data should be structured as a supervised training dataset in a text to text format. Each record or row in the data will contain the input text, in other words, the prompt, which is followed by the expected output of the model. This means that the model can be tuned for a task that can be modeled as a text-to-text problem. 
            * ![](img/10/23.flow.PNG)
            8. After specifying the path to your dataset, you can start the tuning job and monitor the status in the Google Cloud console. When the tuning job completes, you’ll see the tuned model in the Vertex AI Model Registry and you can deploy it to an endpoint for serving, or you can test it in the Generative AI Studio.

# Lab Get Started with Generative AI Studio

* <ins>Vertex</ins> AI is an end-to-end ML platform that helps you build, deploy, and scale ML models faster and easier. It provides a unified experience for managing all aspects of the ML lifecycle, from data preparation to model deployment.

* <ins>Vertex AI Generative AI Studio</ins> is a cloud-based platform that allows users to create and experiment with generative AI models. The platform provides a variety of tools and resources that make it easy to get started with generative AI, even if you don't have a background in ML.

* use Generative AI Studio with Vertex AI to create prompts and conversations on Google Cloud console, without using the API or Python SDKs.

* <ins>Enable the Vertex AI API</ins>

* <ins>Task 1. Create prompts</ins>
    * ![](img/10/24.lab_create_prompt.jpg)
    1. Artificial Intelligence > Vertex AI > Generative AI Studio > Language > Create prompt
    2. <ins>Create Prompt</ins> lets you designs prompts for tasks relevant to your business use case including code generation. 
    * ![](img/10/25.lab_overview_create_prompt.jpg)
    3. quick overview of the interface.
    4. <ins>Prompt design</ins> - feed input text, e.g. a question, to the model. A response based on how you structured your prompt. The process of figuring out and designing the best input text (prompt) to get the desired response back from the model is called <ins>Prompt Design</ins>. 
        1. 3 methods to shape the model's response.
        2. <ins>Zero-shot prompting</ins> - This is a method where the LLM is given no additional data on the specific task that it is being asked to perform. Instead, it is only given a prompt that describes the task. For example, if you want the LLM to answer a question, you just prompt "what is prompt design?".
        3. <ins>One-shot prompting</ins> - This is a method where the LLM is given a single example of the task that it is being asked to perform. For example, if you want the LLM to write a poem, you might give it a single example poem.
        4. <ins>Few-shot prompting</ins> - This is a method where the LLM is given a small number of examples of the task that it is being asked to perform. For example, if you want the LLM to write a news article, you might give it a few news articles to read.
    5. the FREE-FORM and STRUCTURED tabs  - use when designing your prompt.
        1. <ins>FREE-FORM</ins> - This mode provides a free and easy approach to design your prompt. It is suitable for small and experimental prompts with no additional examples. You will be using this to explore zero-shot prompting.
        2. <ins>STRUCTURED</ins> - This mode provides an easy-to-use template approach to prompt design. Context and multiple examples can be added to the prompt in this mode. This is especially useful for one-shot and few-shot prompting methods which you will be exploring later.

    ![](img/10/26.lab_free-form.jpg)
    6. <ins>FREE-FORM mode</ins> - try zero-shot prompting
        1. Input : What is a prompt gallery? > SUBMIT > The model will respond to a comprehensive definition of the term prompt gallery.
        2. Some exploratory exercises to explore.  Inspect if how the responses change as to change the parameters?
            * ![](img/10/1.token_limit.PNG)
            1. adjust the Token limit parameter to 1 -> SUBMIT. 
            * ![](img/10/2.token_limit_1024.PNG)
            2. Token limit : 1024
            * ![](img/10/3.temperature_5.PNG)
            3. Temperature : 0.5
            * ![](img/10/4.temperature_1.PNG)
            4. Temperature : 1.0
        
    7. <ins>STRUCTURED mode</ins> - design prompts in more organized ways. Provide Context and Examples in their respective input fields. This is a good opportunity to learn one-shot and few-shot prompting.
        1. In this section, you will ask the model to complete a sentence. 
        2. Create Prompt > STRUCTURED tab > Test > INPUT : the color of the sky is > SUBMIT.
        * [](img/10/27.lab_structured.jpg)
        3. Instead of completing the sentence, the model gave a full sentence as a response which is not what we wanted. You can try to influence the model's response with one-shot prompting. This time around you will add an example for the model to base its output from.
        * [](img/10/28.lab_structured_example.jpg)
        5. Examples > INPUT : the color of the grass is > OUTPUT > green > SUBMIT > Now the model will respond to complete the sentence instead. The response should be something like this. You have successfully influenced the way the model produces response.

    8. Perform sentiment analysis on a sentence, such as determining whether a movie review is positive or negative.
        * ![](img/10/29._lab_sentiment.jpg)
        1. Create Prompt > Examples > Test (delete all text) > INPUT : It was a time well spent! > SUBMIT >  the model did not have enough information to know whether you were asking it to do sentiment analysis. This can be improved by providing the model with a few examples of what you are looking for.
        * ![](img/10/30.lab_more_example.jpg)
        2. adding these examples (refer image) > Submit
            ``` mark
            INPUT	                            OUTPUT
            A well-made and entertaining film	positive
            I fell asleep after 10 minutes	    negative
            The movie was ok	                neutral
            ```
        3. The model now provides a sentiment for the input text : It was a time well spent!, the sentiment is labeled as positive.

    * ![](img/10/31.save_prompt.jpg)
    9. save the newly designed prompt - Click save >  name: sentiment analysis > SAVE button > 
    * ![](img/10/32.saved_prompt.jpg)
    10. The saved prompt will appear at the MY PROMPTS tab.

* ![](img/10/32.conversation.jpg)
* <ins>Task 2. Create conversations</ins> - a freeform chat with the model, which tracks what was previously said and responds based on context.
    * ![](img/10/33.chat_prompt.jpg)
    1. Language > CREATE CHAT PROMPT > add context to the chat and the model respond based on the context provided.
    2. Context field : Your name is Roy. You are a support technician of an IT department. You only respond with "Have you tried turning it off and on again?" to any queries. | the chatbox under Responses : My computer is so slow | Send message.
    * ![](img/10/34.explore.jpg)
    
* ![](img/10/35.gallery.jpg)
* <ins>Task 3. Explore prompt gallery</ins> - explore how generative AI models can work for a variety of use cases.
    * Language > Prompt Gallery(Examples) > Play from Summarization, Classification, Extraction, Writing, and Ideation, and explore them at your own pace.
    * ![](img/10/5.interview.PNG)
    * ![](img/10/6.ad.PNG)
    * ![](img/10/7.pixel.PNG)
    * ![](img/10/8.question.PNG)
