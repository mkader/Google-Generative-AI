<h1>Attention Mechanism: Overview</h1>

* Want to translate in English sentence "the cat ate the mouse" to French.
	* use an encoder decoder. This is a popular model that is used to translate sentences.
	
	![alt img](images/3/am_1.PNG)
	* The encoder decoder takes one word at a time and translates it at each time step.
	
	![alt img](images/3/am_2.PNG)
	* However, sometimes the words in the source language do not align with the words in the target language.
		* Here's an example. Take the sentence "Black cat ate the mouse". 
		* the first English word is black. However, in the translation, the first French word is chat, which means cat in English.
	
![alt img](images/3/am_3.PNG)
* how can you train a model to focus more on the word cat instead of the word black?
	* to improve the translation. You can add the attention mechanism to the encoder decoder.

* <b>Attention mechanism</b> is a technique that allows the NN to focus on specific parts of an input sequence.
	* This is done by assigning weights to different parts of the input sequence with the most important parts receiving the highest weights.

![alt img](images/3/am_4.PNG)
* a <b>traditional RNN based encoder decoder</b> looks like.
	* The model takes one word at a time as input updates the hidden state and passes it on to the next time step.
	* In the end, only the final hidden state is passed on to the decoder.
	* The decoder works with the final hidden state for processing and translates it to the target language.

* An attention model differs from the traditional sequence to sequence model in two ways.
	* 1.the encoder passes a lot more data to the decoder. 
		* So instead of just passing the final hidden state number three to the decoder, the encoder passes all the hidden states from each time step. 
		* This gives the decoder more context beyond just the final hidden state.
		* The decoder uses all the hidden state information to translate the sentence.

	![alt img](images/3/am_5.PNG)
	* 2.extra step to the attention decoder before producing its output.
		* steps are to focus only on the most relevant parts of the input.The decoder does the following.
		* 1.looks at the set of encoder states that it has received. Each encoder Hidden State is associated with a certain word in the input sentence.
		* 2.it gives each hidden state a score.
		* 3.multiplies each hidden state by its soft-max score as shown here. Thus amplifying hidden states with the highest scores and downsizing hidden states with low scores.

![alt img](images/3/am_6.PNG)
* If we connect all of these pieces together, we're going to see how the Attention Network works.
	* a represents the attention rate at each time step.
	* H represents the hidden state of the encoder RNN at each time step 
	* h subscript B represents the hidden state of the decoder RNN at each time step.

* With the attention mechanism the inversion of the Black Cat translation is clearly visible in the attention diagram and ate translates as two words, "a mange", in French.
	* During the attention step we use the encoder hidden states and the H4 vector to calculate a context vector a four for this time step. 
	* This is the weighted sum. (red lines below a h)
	* We then concatenate H4 and a 4 into one vector.
	* This concatenated vector is passed through a feedforward neural network.
	* One train jointly with the model to predict the next work.
	* The output of the feedforward neural network indicates the output word of this time step.
	* This process continues till the end of sentence token is generated by the decoder.
	* This is how you can use an attention mechanism to improve the performance of a traditional encoder decoder architecture.
![alt img](images/3/am_7.PNG)

Quiz

1. What are the two main steps of the attention mechanism?
	* [x] Calculating the attention weights and generating the context vector
	* [ ] Calculating the context vector and generating the attention weights
	* [ ] Calculating the attention weights and generating the output word
	* [ ] Calculating the context vector and generating the output word

2. What is the advantage of using the attention mechanism over a traditional recurrent neural network (RNN) encoder-decoder?
	* [x] The attention mechanism lets the decoder focus on specific parts of the input sequence, which can improve the accuracy of the translation.
	* [ ] The attention mechanism requires less CPU threads than a traditional RNN encoder-decoder.
	* [ ] The attention mechanism is more cost-effective than a traditional RNN encoder-decoder.
	* [ ] The attention mechanism is faster than a traditional RNN encoder-decoder.

3. How does an attention model differ from a traditional model?
	* [x] Attention models pass a lot more information to the decoder.
	* [ ] The traditional model uses the input embedding directly in the decoder to get more context.
	* [ ] The decoder only uses the final hidden state from the encoder.
	* [ ] The decoder does not use any additional information.
	
4. What is the name of the machine learning technique that allows a neural network to focus on specific parts of an input sequence?
	* [x] Attention mechanism
	* [ ] Convolutional neural network (CNN)
	* [ ] Long Short-Term Memory (LSTM)
	* [ ] Encoder-decoder

5. What is the advantage of using the attention mechanism over a traditional sequence-to-sequence model?
	* [x] The attention mechanism lets the model focus on specific parts of the input sequence.
	* [ ] The attention mechanism lets the model formulate parallel outputs.
	* [ ] The attention mechanism reduces the computation time of prediction.
	* [ ] The attention mechanism lets the model learn only short term dependencies.
	
6. What is the purpose of the attention weights?
	* [x] To assign weights to different parts of the input sequence, with the most important parts receiving the highest weights.
	* [ ] To incrementally apply noise to the input data.
	* [ ] To generate the output word based on the input data alone.
	* [ ] To calculate the context vector by averaging words embedding in the context.
	
7. What is the name of the machine learning architecture that can be used to translate text from one language to another?
	* [x] Encoder-decoder
	* [ ] Convolutional neural network (CNN)
	* [ ] Long Short-Term Memory (LSTM)
	* [ ] Neural network