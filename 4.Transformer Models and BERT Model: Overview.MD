<h1>Transformer Models and BERT Model: Overview</h1>

![alt img](images/4/tm_1.png)

![alt img](images/4/tm_3.png)
* <B>Transformer Models</B>
	* A transformer is an encoder decoder model that uses the attention mechanism. 
	* Attention mechanism helps improve the performance of machine translation applications. Transformer models were built using attention mechanisms at the core. 

![alt img](images/4/tm_2.png)
* A transformer model consists of encoder and decoder.
* The encoder encodes the input sequence and passes it to the decoder and the decoder decodes the representation for a relevant task. 

![alt img](images/4/tm_4.png)
* Each encoder can be broken down into two sub layers. 
	* 1.self attention. The input of the encode are first flows through a self attention layer, which helps to encode or look at relevant parts of the words as it encodes a central word in the input sentence. 
	* 2.feedforward layer. The output of the self attention layer is fed to the feedforward neural network. 

* The decoder has both the self attention and the feedforward layer, but between them is the encoder decoder, attention layer that helps a decoder focus on relevant parts of the input sentence. 
	
![alt img](images/4/tm_5.png)
* The word at each position passes through a self attention process. Then it passes through a feedforward neural network, the exact same network with each vector flowing through it separately. 

![alt img](images/4/tm_6.png)
* In the self attention layer, the input embedding is broken up into query, key, and value vectors.
* These vectors are computed using weights that the transformer learns during the training process. 

![alt img](images/4/tm_7.png)
* All of these computations happen in parallel in the model, in the form of matrix computation.

![alt img](images/4/tm_8.png)
* Once we have the query key and value vectors, the next step is to multiply each value vector by the soft max score in preparation to sum them up.

![alt img](images/4/tm_9.png)
* Next, we have to sum up the weighted value vectors which produces the output of the self attention layer at this position. For the first word, you can send along the resulting vector to the feedforward neural network. 

* To sum up this process of getting the final embeddings, these are the steps that we take. 
	* ![alt img](images/4/tm_10.png), 
	* ![alt img](images/4/tm_11.png)
	* We start with the natural language sentence embed each word in the sentence. 
	* ![alt img](images/4/tm_12.png)
	* After that, we perform multi-headed attention eight times in this case and multiply this embedded word with the respective weighted matrices. 
	* ![alt img](images/4/tm_13.png)
	* We then calculate the attention using the resulting Q K.V. matrices. 
	* ![alt img](images/4/tm_14.png)
	* Finally, we can concatenate the matrices to produce the output matrix, which is the same dimension as the final matrix that this layer initially got. 

![alt img](images/4/tm_15.png)
* There's multiple variations of transformers out there now. 
	* Some use both the encoder and the decoder component from the original architecture.
	* Some use only the encoder and some use only the decoder. 
	* A popular encoder only architecture is Bert.

![alt img](images/4/tm_16.png)
* BERT (bidirectional encoder representations from transformers) Model 
![alt img](images/4/tm_17.png)

* The way that Bert works is that it was trained on two different tasks. 
	![alt img](images/4/tm_18.png)
	* Task one is called a masked language model, where the sentences are masked and the model is trained to predict the masked words.
		* If you were to train Bert from scratch, you would have to mask a certain percentage of the words in your corpus. The recommended percentage for masking is 15%. 
		* The masking percentage achieves a balance between too little and too much masking. 
		* Too little masking makes the training process extremely expensive, and 
		* too much masking removes the context of the model requires. 
	![alt img](images/4/tm_19.png)
	* The second task is to predict the next sentence.
		* For example, the model is given two sets of sentences. Bert aims to learn the relationships between sentences and predict the next sentence given the first one. 
		* For example, sentence A could be a man went to the store and sentence B is he bought a gallon of milk. 
		* Bert is responsible for classifying if sentence B is in next sentence after sentence A. This is a binary classification task. This helps Bert perform at a sentence level in order to train Bert. 

![alt img](images/4/tm_20.png)
* Need to feed three different kinds of embeddings to the model for the input sentence: token, segment and position embeddings. 
	* The token embeddings is a representation of each token as an embedding in the input sentence.
		* The words are transformed into vector representations of certain dimensions.
		* Bert can solve NLP tasks that involve tex classification as well. 
			* An example is to classify whether two sentences say my dog is cute and he likes playing are semantically similar.
			* The pairs of input texts are simply concatenated and fed into the model. 
	* How does Bert distinguish the input in a given pair? The answer is to use segment embeddings.
		* There is a special token represented by SEP that separates the two different splits of the sentence. 
	* Another problem is to learn the order of the words in the sentence.
		* As you know, Bert consists of a stack of transformers. Bert is designed to process input sequences up to a length of 512. 
		* The order of the input sequence is incorporated into the position embeddings.
		* This allows Bert to learn a vector representation for each position. 

![alt img](images/4/tm_21.png)
* Bert can be used for different downstream tasks. Although Bert was trained on mass language modeling and single sentence classification, it can be used for popular NLP tasks like single sentence classification. Sentence Pair Classification. Question Answering and single sentence tagging tasks. 


Summary

* The transformer model, uses attention mechanisms to capture the context and dependencies between words in a text. It consists of an encoder and a decoder. The encoder encodes the input sequence using self-attention and a feedforward layer. The decoder also employs self-attention and a feedforward layer, with an additional encoder-decoder attention layer to focus on relevant parts of the input sentence.

* the process of self-attention, where input embeddings are divided into query, key, and value vectors, which are then multiplied and summed to obtain the output of the self-attention layer. The transformer model performs multi-headed attention multiple times, and the resulting matrices are concatenated to produce the final output.

* The BERT model, is a powerful variant of the transformer model and was trained on tasks such as masked language modeling and next sentence prediction. BERT comes in two versions: BERT Base with 12 transformer layers and BERT Large with 24 transformer layers. It can handle long input context and performs well at both the sentence and token level.

* To train BERT, three types of embeddings are used: token embeddings, segment embeddings, and position embeddings. Token embeddings represent each token in the input sentence, segment embeddings distinguish different splits of the sentence, and position embeddings capture the order of the words.

* BERT can be applied to various NLP tasks, including text classification, sentence pair classification, question answering, and single sentence tagging.
